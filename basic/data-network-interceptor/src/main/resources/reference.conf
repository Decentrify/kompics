kompics.net.data {
    //selectionPolicy = "se.sics.kompics.network.data.policies.RandomSelection"
    selectionPolicy = "se.sics.kompics.network.data.policies.AlternatingSelection"
    ratioPolicy = "se.sics.kompics.network.data.policies.TDRatioLearner"
    queueLength = 20
    td {
        alpha = 0.5 // step size for estimate adjustments
        gamma = 0.5 // inverse discount of new state estimate (1 -> no discount, 0 -> ignore new state)
        lambda = 0.85 // trace decay (1 -> ~Monte Carlo, 0 -> TD(0) ~ one-step TD) 
        stepSize = 5 // inverse of state resolution (a value of n will result in 2n+1 states)
        actionValueEstimator = "FUNCTION" // implementation for Q(s, a) (in ["MATRIX", "COLLAPSED", "FUNCTION"])
        actions = [1, 2] // possible actions (were i represents a step of i*1/stepSize) [x, y, z] autocompletes to [-z, -y, -x, 0, x, y, z] for example
        epsilonGreedy {
            epsilon = 0.3 // initial probability of selecting randomly instead of greedy
            epsilonDelta = 0.01 // epsilon reduction after every action choice (defines temperature decay)
            minEpsilon = 0.1 // minimal temperature (setting to 0 means convergence to pure greedy over time)
        }
    }
}
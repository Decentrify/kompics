kompics.net.data {
    //selectionPolicy = "se.sics.kompics.network.data.policies.RandomSelection"
    selectionPolicy = "se.sics.kompics.network.data.policies.AlternatingSelection"
    ratioPolicy = "se.sics.kompics.network.data.policies.TDRatioLearner"
    queueLength = 20
    td {
        alpha = 0.1 // step size for estimate adjustments
        gamma = 1.0 // inverse discount of new value (1 -> no discount, 0 -> ignore new value)
        stepSize = 5 // inverse of state resolution (a value of n will result in 2n+1 states)
        actionValueEstimator = "COLLAPSED" // implementation for Q(s, a)
        actions = [1, 2] // possible actions (were i represents a step of i*1/stepSize) [x, y, z] autocompletes to [-z, -y, -x, 0, x, y, z] for example
        epsilonGreedy {
            epsilon = 0.5 // initial probability of selecting randomly instead of greedy
            epsilonDelta = 0.01 // epsilon reduction after every action choice (defines temperature decay)
            minEpsilon = 0.05 // minimal temperature (setting to 0 means convergence to pure greedy over time)
        }
    }
}